{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !pip install transformers\n",
        "# !pip install -U sentence-transformers\n",
        "# !pip install wordsegment\n",
        "# !pip install pyvi\n",
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwLriMDmp0pw",
        "outputId": "8e164586-1548-41ec-e13c-43799f7977ec"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYxMNCQ8om-i"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
        "print(tokenizer(\"hôm nay Tôi Đi hỌc\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# sau khi run xong bước này, vào biểu tượng file bên phải -> vào drive/MyDrive/Colab Notebooks, tạo folder tên dataset -> upload 2 file dataset vào thư mục này"
      ],
      "metadata": {
        "id": "rTgnHHCz3d3S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1c7e94a-3a95-4a5a-fa20-734adf080049"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import string\n",
        "import tensorflow as tf\n",
        "\n",
        "# Spacy for english word segmentation\n",
        "import spacy\n",
        "from spacy.tokenizer import Tokenizer\n",
        "from spacy.lang.en import English\n",
        "# Pyvi for vietnam word segmentation\n",
        "from pyvi import ViTokenizer\n",
        "\n",
        "# import pandas as pd\n",
        "import os\n",
        "import io\n",
        "import string\n",
        "import re"
      ],
      "metadata": {
        "id": "7KRZVQucTZ9Y"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !ls \"/content/drive/MyDrive/Colab Notebooks/dataset/en_sents\"\n",
        "dataset_path = \"/content/drive/MyDrive/Colab Notebooks/dataset/\"\n",
        "spacy_en = spacy.load('en_core_web_sm')\n",
        "\n",
        "eos = '<eos>'\n",
        "bos = '<bos>'\n",
        "\n",
        "#  scr - tar - model_eng-viet save\n",
        "#  scr - tar - model_viet-eng save\n",
        "# ?train bert - phobert\n",
        "# def create_raw_dataset():\n",
        "#     data_dir = \"/content/drive/MyDrive/Colab Notebooks/dataset/\"\n",
        "#     en_sents = open(data_dir + 'en_sents', \"r\").read().splitlines()\n",
        "#     vi_sents = open(data_dir + 'vi_sents', \"r\").read().splitlines()\n",
        "#     return {\n",
        "#         \"English\": [line for line in en_sents[:5000]],\n",
        "#         \"Vietnamese\": [line for line in vi_sents[:5000]],\n",
        "#     }\n",
        "# raw_data = create_raw_dataset()\n",
        "# df = pd.DataFrame(raw_data, columns=[\"English\", \"Vietnamese\"])\n",
        "# print(df)"
      ],
      "metadata": {
        "id": "-TCbEiH73orD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_en(en_dtset):\n",
        "  en_return = []\n",
        "  for sent in en_dtset:\n",
        "    en_return.append([tok.text for tok in spacy_en.tokenizer(sent)])\n",
        "\n",
        "  # add begin and end to sentence\n",
        "  # for i in range(len(en_return)):\n",
        "  #   en_return[i].insert(0, bos)\n",
        "  #   en_return[i].insert(len(en_return[i]), eos)\n",
        "  return en_return\n",
        "\n",
        "def tokenize_vi(vi_dtset):\n",
        "  vi_return = []\n",
        "  for sent in vi_dtset:\n",
        "    vi_return.append(ViTokenizer.tokenize(sent).split())\n",
        "\n",
        "  # add begin and end to sentence\n",
        "  # for i in range(len(vi_return)):\n",
        "  #   vi_return[i].insert(0, bos)\n",
        "  #   vi_return[i].insert(len(vi_return[i]), eos)\n",
        "  return vi_return\n",
        "\n",
        "en_sentences = open(dataset_path + 'en_sents', 'r').read().splitlines()\n",
        "en_lst = [line.lower() for line in en_sentences[:10000]]\n",
        "\n",
        "vi_sentences = open(dataset_path + 'vi_sents', 'r').read().splitlines()\n",
        "vi_lst = [line.lower() for line in vi_sentences[:10000]]\n",
        "\n",
        "en_seg = tokenize_en(en_lst)\n",
        "# print(en_seg)\n",
        "\n",
        "vi_seg = tokenize_vi(vi_lst)\n",
        "vi_seg\n"
      ],
      "metadata": {
        "id": "QDtUpZHXp8BX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Word embedding\n",
        "# from gensim.models import Word2Vec\n",
        "\n",
        "# vi_model = Word2Vec(vi_seg, size=100, window=2, min_count=1, sample=0.0001, workers=4, sg=0, negative=10, cbow_mean=1, iter=5)\n",
        "\n",
        "class Language():\n",
        "    def __init__(self, lines):\n",
        "        self.lines = lines\n",
        "        self.word2id = {}\n",
        "        self.id2word = {}\n",
        "        self.vocab = set()\n",
        "        self.max_len = 0\n",
        "        self.min_len = 0\n",
        "        self.vocab_size = 0\n",
        "        self.init_language_params()\n",
        "\n",
        "    def init_language_params(self):\n",
        "        self.word2id['<pad>'] = 0\n",
        "        for line in self.lines:\n",
        "            self.vocab.update(line)\n",
        "        for id, word in enumerate(self.vocab):\n",
        "            self.word2id[word] = id + 1\n",
        "        for word, id in self.word2id.items():\n",
        "            self.id2word[id] = word\n",
        "        \n",
        "        self.max_len = max([len(line) for line in self.lines])\n",
        "        self.min_len = min([len(line) for line in self.lines])\n",
        "        self.vocab_size = len(self.vocab) + 1\n",
        "    \n",
        "    def sent_to_vec(self, sent):\n",
        "      return np.array([self.word2id[word] for word in sent])\n",
        "\n",
        "\n",
        "scr_en = Language(en_seg)\n",
        "tar_vi = Language(vi_seg)\n",
        "\n",
        "scr_vec = [scr_en.sent_to_vec(sent) for sent in scr_en.lines]\n",
        "tar_vec = [tar_vi.sent_to_vec(sent) for sent in tar_vi.lines]\n",
        "\n",
        "scr_tensor = tf.keras.preprocessing.sequence.pad_sequences(scr_vec, scr_en.max_len, padding='post')\n",
        "tar_tensor = tf.keras.preprocessing.sequence.pad_sequences(tar_vec, tar_vi.max_len, padding='post')\n",
        "\n",
        "scr_tensor"
      ],
      "metadata": {
        "id": "IoRiJ7BOpT8q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "063e53e3-8f75-4c50-9520-cdb73e857ce0"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[4954, 1255, 5273, ...,    0,    0,    0],\n",
              "       [3877, 2855, 4257, ...,    0,    0,    0],\n",
              "       [5116, 4516,    0, ...,    0,    0,    0],\n",
              "       ...,\n",
              "       [ 951, 4944, 3361, ...,    0,    0,    0],\n",
              "       [ 951, 1326, 4202, ...,    0,    0,    0],\n",
              "       [ 979, 4646,  328, ...,    0,    0,    0]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install transformers && pip install torch"
      ],
      "metadata": {
        "id": "mKE8DZn8RRJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import DistilBertTokenizerFast, DistilBertModel\n",
        "\n",
        "# tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
        "# tokens = tokenizer.encode('This is a input.', return_tensors='pt')\n",
        "# print(\"These are tokens!\", tokens)\n",
        "# for token in tokens[0]:\n",
        "#     print(\"This are decoded tokens!\", tokenizer.decode([token]))"
      ],
      "metadata": {
        "id": "hoqpLkaLRSVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "# print(model.embeddings.word_embeddings(tokens))\n",
        "# for e in model.embeddings.word_embeddings(tokens)[0]:\n",
        "#     print(\"This is an embedding!\", e)"
      ],
      "metadata": {
        "id": "_RdyjitlRm7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sentence_transformers import SentenceTransformer, models\n",
        "\n",
        "# ## Step 1: use an existing language model\n",
        "# word_embedding_model = models.Transformer('distilroberta-base')\n",
        "\n",
        "# ## Step 2: use a pool function over the token embeddings\n",
        "# pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
        "# # print('pool: ', pooling_model)\n",
        "\n",
        "# ## Join steps 1 and 2 using the modules argument\n",
        "# model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
        "# print('model: ', model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jR6l5BPx8Em8",
        "outputId": "8f7b53c2-62df-47e0-f694-fa0f14974c2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model:  SentenceTransformer(\n",
            "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: RobertaModel \n",
            "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "drm5S8ArDsjj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}